{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA2001 Assignment - Assessing SA2 regions\n",
    "### Authors: ykim4904, unikey2, unikey3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Clean and Import datasets into SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary datasets for the report + initial filtering\n",
    "sa2_bounds_raw = gpd.read_file(\"space_data/SA2.shp\")\n",
    "businesses_raw = pd.read_csv(\"other_data/Businesses.csv\")\n",
    "stops_raw = pd.read_csv(\"other_data/Stops.txt\")\n",
    "polls_raw = pd.read_csv(\"other_data/PollingPlaces2019.csv\")\n",
    "schools_prima_raw = gpd.read_file(\"space_data/catchments/catchments_primary.shp\")\n",
    "schools_secon_raw = gpd.read_file(\"space_data/catchments/catchments_secondary.shp\")\n",
    "schools_futur_raw = gpd.read_file(\"space_data/catchments/catchments_future.shp\")\n",
    "populations_raw = pd.read_csv(\"other_data/Population.csv\")\n",
    "incomes_raw = pd.read_csv(\"other_data/Income.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection, querying functions\n",
    "\n",
    "credentials = \"Credentials.json\"\n",
    "\n",
    "def pgconnect(credential_filepath, db_schema=\"public\"):\n",
    "    with open(credential_filepath) as f:\n",
    "        db_conn_dict = json.load(f)\n",
    "        host       = db_conn_dict['host']\n",
    "        db_user    = db_conn_dict['user']\n",
    "        db_pw      = db_conn_dict['password']\n",
    "        default_db = db_conn_dict['database']\n",
    "        try:\n",
    "            db = create_engine('postgresql+psycopg2://'+db_user+':'+db_pw+'@'+host+'/'+default_db, echo=False)\n",
    "            conn = db.connect()\n",
    "            print('Connected successfully.')\n",
    "        except Exception as e:\n",
    "            print(\"Unable to connect to the database.\")\n",
    "            print(e)\n",
    "            db, conn = None, None\n",
    "        return db,conn\n",
    "\n",
    "def query(conn, sqlcmd, args=None, df=True):\n",
    "    result = pd.DataFrame() if df else None\n",
    "    try:\n",
    "        if df:\n",
    "            result = pd.read_sql_query(sqlcmd, conn, params=args)\n",
    "        else:\n",
    "            result = conn.execute(sqlcmd, args).fetchall()\n",
    "            result = result[0] if len(result) == 1 else result\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered: \", e, sep='\\n')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd -> PostGIS suitable type conversino helper function\n",
    "def create_wkt_element(geom, srid):\n",
    "    if geom.geom_type == 'Polygon':\n",
    "        geom = MultiPolygon([geom])\n",
    "    return WKTElement(geom.wkt, srid)\n",
    "srid = 4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, conn = pgconnect(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "print(inspect(db).get_schema_names())\n",
    "conn.execute(\"CREATE SCHEMA IF NOT EXISTS DATA2001_A_IEY;\")\n",
    "conn.execute(\"SET search_path TO DATA2001_A_IEY, public;\")\n",
    "print(query(conn, \"SELECT PostGIS_version();\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.1 - Cleaning dataset : SA2 Regions' Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial filtering (SA2_regions)\n",
    "# 1. W're only interested in \"Greater Sydney\" GCC)\n",
    "print(sa2_bounds_raw.GCC_NAME21.value_counts().head())\n",
    "print(\"... more regions (truncated)\\n\")\n",
    "\n",
    "sa2_bounds = sa2_bounds_raw[sa2_bounds_raw.GCC_NAME21 == \"Greater Sydney\"]\n",
    "print(sa2_bounds.GCC_NAME21.value_counts(), \"\\n\")\n",
    "# 2. We'll only be conducting analysis on SA2 regions. We'll not be \n",
    "# examining ins encompassing(broader) regions such as SA3, SA4, and states. \n",
    "sa2_bounds = sa2_bounds.loc[:, [\"SA2_CODE21\", \"SA2_NAME21\", \"AREASQKM21\", \"geometry\"]]\n",
    "# 3. rename columns\n",
    "sa2_bounds = sa2_bounds.rename(\n",
    "    columns = dict(SA2_CODE21=\"code\", SA2_NAME21=\"name\", AREASQKM21=\"area_sq_km\"))\n",
    "# 4. cast appropriately\n",
    "sa2_bounds[\"code\"] = sa2_bounds[\"code\"].astype(int)\n",
    "sa2_bounds[\"name\"] = sa2_bounds[\"name\"].astype(str)\n",
    "sa2_bounds[\"geom\"] = sa2_bounds['geometry'].apply(lambda x: create_wkt_element(geom=x,srid=srid))\n",
    "sa2_bounds = sa2_bounds.drop(columns=\"geometry\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.2 - Cleaning datasets : Businesses, Train-Stops, and Polling-Places(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = businesses_raw\n",
    "businesses = businesses = businesses.loc[:, [\"industry_name\", \"sa2_code\", \"total_businesses\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stops_raw\n",
    "# filter out only those in greater sydney\n",
    "stops = stops[((stops['stop_lon'] > 150.4) & (stops['stop_lon'] < 151.4))]\n",
    "stops = stops[((stops['stop_lat'] > -34.2) & (stops['stop_lat'] < 33.5))]\n",
    "stops = stops.loc[:, [\"stop_id\", \"stop_name\", \"stop_lon\", \"stop_lat\"]]\n",
    "stops['stop_loc'] = gpd.points_from_xy(stops.stop_lon, stops.stop_lat)\n",
    "stops[\"stop_loc\"] = stops['stop_loc'].apply(lambda x: WKTElement(x.wkt, srid=srid))\n",
    "stops = stops.drop(columns=[\"stop_lon\", \"stop_lat\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polls = polls_raw\n",
    "polls = polls.rename(columns=dict(polling_place_id=\"poll_id\", polling_place_name=\"poll_name\"))\n",
    "polls = polls.loc[:, [\"poll_id\", \"poll_name\", \"latitude\", \"longitude\"]]\n",
    "polls = polls.dropna()\n",
    "polls[\"poll_loc\"] = gpd.points_from_xy(polls.longitude, polls.latitude)\n",
    "polls[\"poll_loc\"] = polls['poll_loc'].apply(lambda x: WKTElement(x.wkt, srid=srid))\n",
    "polls = polls.drop(columns=[\"longitude\", \"latitude\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.3 - Cleaning Datasets: Population and Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = populations_raw\n",
    "population[\"young_people\"] = population.iloc[:, 2:5+1].sum(axis=1)\n",
    "population = population.loc[:, [\"sa2_code\", \"young_people\", \"total_people\"]]\n",
    "\n",
    "income = incomes_raw\n",
    "income = income.loc[:, [\"sa2_code\", \"median_income\"]]\n",
    "income = income[income.median_income.str.startswith('np') == False]\n",
    "income['median_income'] = income['median_income'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.4 - Cleaning dataset: School Catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.5 - Cleaning Dataset (Task 3) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.6 - Cleaning Dataset (Task 3) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.7 - Cleaning Dataset (Task 3) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.8 - Verify Cleaning work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sa2_bounds.info())\n",
    "# sa2_bounds.head()\n",
    "# print(businesses.info())\n",
    "# businesses.head()\n",
    "# print(stops.info())\n",
    "# stops.head()\n",
    "# print(polls.info())\n",
    "# polls.head()\n",
    "# print(population.info())\n",
    "# population.head()\n",
    "# print(income.info())\n",
    "# income.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Importing into PSQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_f = open(\"schema_init.sql\", \"r\")\n",
    "schema_definition = schema_f.read()\n",
    "conn.execute(schema_definition)\n",
    "\n",
    "sa2_bounds.to_sql('sa2_bounds', conn, if_exists='append', index=False, dtype={'geom': Geometry('MULTIPOLYGON', srid)})\n",
    "businesses.to_sql('businesses', conn, if_exists='append', index=False)\n",
    "stops.to_sql('stops', conn, if_exists='append', index=False, dtype={'stop_loc': Geometry('POINT', srid)})\n",
    "polls.to_sql('polls', conn, if_exists='append', index=False, dtype={'poll_loc': Geometry('POINT', srid)})\n",
    "population.to_sql('population', conn, if_exists='append', index=False)\n",
    "income.to_sql('income', conn, if_exists='append', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_z_score(df, column):\n",
    "    df[\"z_score\"] = (df[column] - df[column].mean()) / df[column].std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Score metric 1 - Retail businesses per 1000 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Score metric 2 - Health businesses per 1000 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Score metric 3 - Public Transport Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT code AS \"sa2_code\", COUNT(code) AS \"stops_count\"\n",
    "FROM sa2_bounds LEFT JOIN stops \n",
    "    ON (ST_Contains(geom, stop_loc))\n",
    "GROUP BY code\n",
    "'''\n",
    "stops_num = query(conn, sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_z_score(stops_num, 'stops_count')\n",
    "stops_num_scr = stops_num.rename(columns=dict(stops_count='raw_score'))\n",
    "stops_num_scr['score_desc'] = \"# of train/bus stops\"\n",
    "stops_num_scr.to_sql('score_table', conn, if_exists='append', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Score metric 4 - Polling Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT code, COUNT(code) AS \"poll_count\"\n",
    "FROM sa2_bounds, polls\n",
    "WHERE ST_Contains(geom, poll_loc)\n",
    "GROUP BY(code)\n",
    "'''\n",
    "polls_count = query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT * FROM sa2_bounds, polls\n",
    "WHERE ST_Contains(geom, poll_loc);\n",
    "'''\n",
    "query(conn, sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
